{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Engineering Capstone Project\n",
    "### Project Summary\n",
    "\n",
    "In this project, the key objective of this project is create an ETL pipline from  I94 Immigration\n",
    "is figre out how factors affect the number of tourists,some factors is :\n",
    "- the temperature\n",
    "- the reasonality of travel\n",
    "- the number of entry ports\n",
    "- the demographics of various citis.\n",
    "\n",
    "Data can be used to analyse immigration flow to and from US through different airports. It's used a star schema with a facts table an dimensional tables.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import os\n",
    "import pandas as pd\n",
    "import configparser\n",
    "import csv\n",
    "import geopandas\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# For Spark lib\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DoubleType as Double, StringType as Str, IntegerType as Int,\\\n",
    "    TimestampType as Timestamp, DateType as Date, LongType as Long\n",
    "from pyspark.sql.functions import isnan, when, count, col, udf, dayofmonth, dayofweek, month, year, weekofyear\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Scope the Project and Gather Data\n",
    "### Project Scope \n",
    "In this project, I will utilize the PySpark to perform an extract data , transform data and load ETL pipline.\n",
    "The ETL pipeline is base on the following steps:\n",
    "    Collect data from Database --> Processing data --> Cleaning data --> Storing data to the Data warehourse.\n",
    "The output of ETL is Star Schema model to parquet files and it can be store in local system,AWS Redshift,Cloud Database or ..etc..\n",
    "The main tools is pandas,pyspark \n",
    "\n",
    "### Describe and Gather Data \n",
    "The project is using the datasets:\n",
    " - Immigration Data: A data dictionary is included in the workspace.This data comes from the U.S. National Tourism and Trade Office. More information on the immigration data [here](https://travel.trade.gov/research/reports/i94/historical/2016.html).\n",
    " - US Cities Demographic: the data of the demographic of all US cities and census-designated places with a population greater or equal to 65,000. Dataset comes from OpenSoft found [here](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/)\n",
    " - Global Land Temperatures By City: the data of the temperatures of various cities in the world from  1743 to 2013. This dataset came from Kaggle found [here](https://www.kaggle.com/datasets/berkeleyearth/climate-change-earth-surface-temperature-data).\n",
    " - Airport Code Table: Airpot codes data contains information about different airports around the world. The data come from [here](https://datahub.io/core/airport-codes#data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Configuration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('AWS.cfg')\n",
    "DATA_FOLDER = \"./data/\"\n",
    "RESULT_FOLDER = \"./data/result/\"\n",
    "SAS_LableFile_path = DATA_FOLDER + 'I94_SAS_Labels_Descriptions.SAS'\n",
    "KEY = config.get('AWS','KEY')\n",
    "SECRET = config.get('AWS','SECRET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Immigration Dataset\n",
    "The immigration dataset is large. Because the project was developed on my PC local so that I already download all data files to store inside the data folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the pyspark python lib, I import the I94 immigration dataset to Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_immigration_sp=spark.read.parquet(\"./data/sas_data\")\n",
    "df_immigration_sp.printSchema()\n",
    "df_immigration_sp.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding data dictinaries\n",
    "In this step, I add more datasets from the data dictionary. First, I read the I94_SAS_Labels_Descriptions.SAS file includes complex data such as country name, VISA type, airport name, airport mode,..etc.Therefore, I will run some processes to extract CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_data_from_SAS_labels(input_label):\n",
    "    '''\n",
    "    A procedure that returns a cleaned list of code data pairs for the provided input label\n",
    "    Parameters:\n",
    "        Input:\n",
    "        input_label : str\n",
    "            name of the label in the SAS labels descriptions file\n",
    "        \n",
    "        Returns:\n",
    "        code_data_list : list(tuple(str, str))\n",
    "            a list of code data pairs extracted from the SAS labels descriptions file and cleaned\n",
    "    '''\n",
    "    with open(SAS_LableFile_path) as labels_descriptions:\n",
    "            raw_labels = labels_descriptions.read()\n",
    "    # extract only label data\n",
    "    labels = raw_labels[raw_labels.index(input_label):]\n",
    "    labels = labels[:labels.index(';')]\n",
    "    # in each line remove unnecessary spaces and extract the code and its corresponding value \n",
    "    lines = labels.splitlines()\n",
    "    code_data_list = []\n",
    "    # In case the input_label is I94PORT. This is speical dataset need 3 columm.\n",
    "    if input_label == \"I94PORT\": \n",
    "        for line in lines:\n",
    "            try:\n",
    "                code, data = line.split('=')\n",
    "                code = code.strip().strip(\"'\").strip('\"')\n",
    "                data = data.strip().strip(\"'\").strip('\"').strip()\n",
    "                _value1 , _value2 = data.split(',')\n",
    "                _value1 = _value1.strip()\n",
    "                _value2 = _value2.strip()\n",
    "                code_data_list.append((code,_value1,_value2))\n",
    "            except:\n",
    "                pass\n",
    "    else:\n",
    "        for line in lines:\n",
    "            try:\n",
    "                code, data = line.split('=')\n",
    "                code = code.strip().strip(\"'\").strip('\"')\n",
    "                data = data.strip().strip(\"'\").strip('\"').strip()\n",
    "                code_data_list.append((code, data))\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "    return code_data_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list array the input code,name of output CSV files and definition of header for each CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_List_I94_Factor = [\"I94CIT & I94RES\",\"I94PORT\",\"I94MODE\",\"I94ADDR\",\"I94BIR\"]\n",
    "_List_CSV_Extract_FileName = ['i94_country.csv','i94_port.csv','i94_model.csv','i94_state_addrl.csv','i94_visa.csv']\n",
    "_List_CSV_Header = [['code','country_name'],['code','port','state_code'],['code','model'],['code','state'],['code','VISA_Type']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the I94_SAS_Labels_Descriptions.SAS file to extract to CSV files data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(len(_List_I94_Factor)):\n",
    "    _name_file = DATA_FOLDER + _List_CSV_Extract_FileName[index]\n",
    "    csvfile = open(_name_file,'w',encoding='UTF8',newline = '')\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(_List_CSV_Header[index])\n",
    "    writer.writerows(extract_data_from_SAS_labels(_List_I94_Factor[index]))\n",
    "    csvfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using pandas, I import CSV  files from the database to enrich the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_immig_sample = pd.read_csv(DATA_FOLDER + 'immigration_data_sample.csv')\n",
    "i94_Country = pd.read_csv (DATA_FOLDER + 'i94_country.csv')\n",
    "i94_Model= pd.read_csv(DATA_FOLDER + 'i94_model.csv')\n",
    "i94_State = pd.read_csv(DATA_FOLDER + 'i94_state_addrl.csv')\n",
    "i94_VISA = pd.read_csv(DATA_FOLDER + 'i94_visa.csv')\n",
    "df_demographics = pd.read_csv(DATA_FOLDER +'us-cities-demographics.csv', sep=';')\n",
    "df_ariport_Code = pd.read_csv(DATA_FOLDER +'airport-codes_csv.csv')\n",
    "df_temperature = pd.read_csv(DATA_FOLDER +'GlobalLandTemperaturesByCity.csv')\n",
    "i94_Port = pd.read_csv(DATA_FOLDER + 'i94_port.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_immig_sample.shape\n",
    "df_immig_sample.columns\n",
    "df_immig_sample.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demographics.shape\n",
    "df_demographics.columns\n",
    "df_demographics.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ariport_Code.shape\n",
    "df_ariport_Code.columns\n",
    "df_ariport_Code.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temperature.shape\n",
    "df_temperature.columns\n",
    "df_temperature.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Explore and Assess the Data\n",
    "##### Explore and Cleaning the Data \n",
    "This step will identify data quality issues, like missing values, duplicate data, etc. After that, I will  remove unnecessary data for clean data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1. Immigration dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show some informations of the immigration dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_immigration_sp.count()\n",
    "df_immigration_sp.show(3)\n",
    "df_immigration_sp.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let find numnber of NULL value for all columns of immigration data table.\n",
    "df_immigration_sp.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_immigration_sp.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of arrdate and depdate columns have double type and it just show indicate the arrival data and departure day so that the double type is unnecessary\n",
    "I will convert 2 this rows to interger type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert for arrival data column\n",
    "df_imigration_arr= df_immigration_sp.withColumn(\"arrdate\",df_immigration_sp[\"arrdate\"].cast('int'))\n",
    "df_imigration_arr.printSchema()\n",
    "df_imigration_arr.head(3)\n",
    "# Convert for departure data column\n",
    "df_imigration_dep= df_imigration_arr.withColumn(\"depdate\",df_imigration_arr[\"depdate\"].cast('int'))\n",
    "df_imigration_dep.printSchema()\n",
    "df_imigration_dep.head(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a view of the immigration dataset\n",
    "df_imigration_dep.createOrReplaceTempView(\"immigration_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the iccid of the immigration table to see if it can be used as the primary key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imigration_dep.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark.sql(\"\"\"\n",
    "        SELECT COUNT (DISTINCT cicid)\n",
    "        FROM immigration_table\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The port code of i94port has a length is 3 so let's check if it can be applied to the codes in the dataset.\n",
    "spark.sql(\"\"\"\n",
    "            SELECT LENGTH (i94port) AS len\n",
    "            FROM immigration_table\n",
    "            GROUP BY len\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of SAS correspond to the number of days since 1960-01-01 so I keep dataset from 1960-01-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_immigration = spark.sql(\"SELECT *,date_add(to_date('1960-01-01'), arrdate) AS arrival_date FROM immigration_table\")\n",
    "df_immigration.createOrReplaceTempView(\"immigration_table\")\n",
    "df_immigration.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make more clearlly of i94visa columms, I replace the data in the I94VISA columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_immigration = spark.sql(\"\"\"SELECT *, CASE \n",
    "                                        WHEN i94visa = 1.0 THEN 'Business' \n",
    "                                        WHEN i94visa = 2.0 THEN 'Pleasure'\n",
    "                                        WHEN i94visa = 3.0 THEN 'Student'\n",
    "                                        ELSE 'N/A' END AS visa_type\n",
    "                                    FROM immigration_table\"\"\")\n",
    "df_immigration.createOrReplaceTempView(\"immigration_table\")\n",
    "df_immigration.show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_immigration = spark.sql(\"\"\"SELECT *, CASE \n",
    "                        WHEN depdate >= 1.0 THEN date_add(to_date('1960-01-01'), depdate)\n",
    "                        WHEN depdate IS NULL THEN NULL\n",
    "                        ELSE 'N/A' END AS departure_date            \n",
    "                FROM immigration_table\"\"\")\n",
    "df_immigration.createOrReplaceTempView(\"immigration_table\")\n",
    "df_immigration.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check N/A values of arrival_date and departure_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT count(*) FROM immigration_table WHERE arrival_date = 'N/A'\").show()\n",
    "spark.sql(\"SELECT count(*) FROM immigration_table WHERE departure_date = 'N/A'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the relationshiop between departure_date and arrival_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "            SELECT COUNT(*)\n",
    "            FROM immigration_table\n",
    "            WHERE departure_date <= arrival_date\n",
    "        \"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "        SELECT arrival_date, departure_date\n",
    "        FROM immigration_table\n",
    "        WHERE departure_date <= arrival_date\n",
    "        \"\"\").show()\n",
    "spark.sql(\"\"\"\n",
    "        SELECT *\n",
    "        FROM immigration_table\n",
    "        WHERE departure_date >= arrival_date\n",
    "        \"\"\").createOrReplaceTempView(\"immigration_table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check distinct arrival dates\n",
    "spark.sql(\"\"\"\n",
    "            SELECT COUNT (DISTINCT arrival_date) \n",
    "            FROM immigration_table;\n",
    "        \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check distinct departure dates\n",
    "spark.sql(\"\"\"\n",
    "            SELECT COUNT (DISTINCT departure_date) \n",
    "            FROM immigration_table;\n",
    "        \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the common values between the two sets\n",
    "spark.sql(\"\"\"   \n",
    "            SELECT COUNT(DISTINCT departure_date) \n",
    "            FROM immigration_table \n",
    "            WHERE departure_date \n",
    "            IN (SELECT DISTINCT arrival_date FROM immigration_table);\n",
    "        \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data for the various arrival modes\n",
    "spark.sql(\"\"\"\n",
    "            SELECT i94mode, count(*)\n",
    "            FROM immigration_table\n",
    "            GROUP BY i94mode\n",
    "        \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_immigration = spark.sql(\"\"\"SELECT *, CASE \n",
    "                                        WHEN i94mode = 1.0 THEN 'Air' \n",
    "                                        WHEN i94mode = 2.0 THEN 'Sea'\n",
    "                                        WHEN i94mode = 3.0 THEN 'Land'\n",
    "                                        WHEN i94mode = 9.0 THEN 'Not reported'\n",
    "                                        ELSE 'N/A' END AS arrival_modes\n",
    "                                    FROM immigration_table\"\"\")\n",
    "df_immigration.createOrReplaceTempView(\"immigration_table\")\n",
    "df_immigration.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to  our dataset can work with the airports  so I just keep only arrival by Air"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_immigration = spark.sql(\"\"\"SELECT *\n",
    "                                FROM immigration_table\n",
    "                                WHERE arrival_modes = 'Air' \"\"\")\n",
    "df_immigration.createOrReplaceTempView(\"immigration_table\")\n",
    "df_immigration.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check N/A values of i94bir to ensure if there are missing values or not.\n",
    "spark.sql(\"\"\"\n",
    "            SELECT COUNT(*)\n",
    "            FROM immigration_table\n",
    "            WHERE i94bir IS NULL\n",
    "        \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the maximum and minimum values of the biryear\n",
    "spark.sql(\"SELECT  MIN(biryear), MAX(biryear) FROM immigration_table WHERE biryear IS NOT NULL\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency of travellers by birth year\n",
    "spark.sql(\"\"\"\n",
    "            SELECT biryear, COUNT(*)\n",
    "            FROM immigration_table \n",
    "            WHERE biryear IS NOT NULL\n",
    "            GROUP BY biryear\n",
    "            ORDER BY biryear ASC\n",
    "        \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since the birth year is available for each row, we can compute the age. Let's check if computed values match the age\n",
    "spark.sql(\"\"\"   \n",
    "            SELECT (2016-biryear)-i94bir AS difference, count(*) \n",
    "            FROM immigration_table \n",
    "            WHERE i94bir IS NOT NULL \n",
    "            GROUP BY difference\n",
    "        \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the gender to see if the data is useable\n",
    "spark.sql(\"\"\"\n",
    "                SELECT gender, count(*) \n",
    "                FROM immigration_table\n",
    "                GROUP BY gender\n",
    "        \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will filter out all the rows where the gender is missing or incorrect. In this case, I will remove null values,U and A so that the gender just only has 2 values is F: Female and M: Male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_immigration =spark.sql(\"\"\"\n",
    "            SELECT * FROM immigration_table \n",
    "            WHERE gender IN ('F', 'M')\"\"\")\n",
    "df_immigration.createOrReplaceTempView(\"immigration_table\")\n",
    "df_immigration.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check N/A values of residence countries,reported address,citizenship countries and visa type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#residence countries\n",
    "spark.sql(\"\"\"\n",
    "SELECT count(*) \n",
    "FROM immigration_table\n",
    "WHERE i94res IS NULL\n",
    "\"\"\").show()\n",
    "#citizenship countries\n",
    "spark.sql(\"\"\"\n",
    "SELECT count(*) \n",
    "FROM immigration_table\n",
    "WHERE i94cit IS NULL\n",
    "\"\"\").show()\n",
    "#reported address\n",
    "spark.sql(\"\"\"\n",
    "SELECT count(*) \n",
    "FROM immigration_table\n",
    "WHERE i94addr IS NULL\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(*)\n",
    "FROM immigration_table\n",
    "WHERE visatype IS NULL\n",
    "\"\"\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT visa_type, visatype, count(*)\n",
    "FROM immigration_table\n",
    "GROUP BY visa_type, visatype\n",
    "ORDER BY visa_type, visatype\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_immigration = spark.sql(\"\"\"SELECT * FROM immigration_table\"\"\").show(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Temperature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temperature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_temperature.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temperature['Country'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the datafame contains data of 159 countries, this project only needs data of United States. Therefore, I will filter out countries unnecessarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Keep only data for the United States\n",
    "df_temperature = df_temperature[df_temperature['Country']=='United States']\n",
    "df_temperature.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the WWII, the commercial air travel began starting in the 1950s so that I will exclude any data prior to 1950s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all dates prior to 1950\n",
    "df_temperature=df_temperature[df_temperature['dt']>\"1950-01-01\"]\n",
    "df_temperature.shape\n",
    "df_temperature.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the most recent date in the dataset\n",
    "df_temperature['dt'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temperature['AverageTemperature'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check for null value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_null_temperature = df_temperature.isnull().sum()\n",
    "_null_temperature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = (_null_temperature[_null_temperature>0]/df_temperature.shape[0]*100).plot(kind='bar', title=f\"Percent of type of Airport %\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temperature[df_temperature.AverageTemperature.isnull()]\n",
    "df_temperature[df_temperature.AverageTemperatureUncertainty.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure the combination of city and date can be used as a primary key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temperature[(df_temperature['City'] == 'Arlington') & (df_temperature.dt == '1950-03-01')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temperature.shape\n",
    "df_temperature[['City','dt']].drop_duplicates().shape\n",
    "df_temperature[df_temperature[['City','dt']].duplicated()].head()\n",
    "df_temperature[(df_temperature['City'] == 'Arlington') & (df_temperature.dt == '1950-03-01')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Airport data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ariport_Code.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_ariport_Code.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract data of coordinates to 2 parameters is longitude and latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longitude = []\n",
    "latitude = []\n",
    "for data in df_ariport_Code['coordinates']:\n",
    "    arrays= data.split(', ')\n",
    "    longitude.append(float(arrays[0]))\n",
    "    latitude.append(float(arrays[1]))\n",
    "\n",
    "df_ariport_Code.insert(12,'longitude',longitude)\n",
    "df_ariport_Code.insert(13,'latitude',latitude)\n",
    "df_ariport_Code.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shows the position of air port in World Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = geopandas.GeoDataFrame(\n",
    "    df_ariport_Code[['longitude','latitude']], \n",
    "    geometry=geopandas.points_from_xy(df_ariport_Code['longitude'], \n",
    "                                      df_ariport_Code['latitude']))\n",
    "world = geopandas.read_file(geopandas.datasets.get_path('naturalearth_lowres'))\n",
    "ax = world.plot(color='white', edgecolor='black', figsize=(16,12))\n",
    "gdf.plot(ax=ax, color='green', markersize=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the countries where these airports are located\n",
    "df_ariport_Code.groupby('iso_country')['iso_country'].count()\n",
    "_df_ariport_count_ = df_ariport_Code.groupby('iso_region')['iso_region'].count()\n",
    "_df_ariport_count_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check  there is no missing data in the iso_county field.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ariport_Code[df_ariport_Code['iso_country'].isna()].shape\n",
    "df_ariport_Code.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the missing country values to see if the continent data is filled out\n",
    "df_ariport_Code[df_ariport_Code['iso_country'].isna()].groupby('continent')['continent'].count()\n",
    "df_ariport_Code.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since all missing values are in africa, we simly remove them from the dataset\n",
    "df_ariport_Code = df_ariport_Code[df_ariport_Code.iso_country.fillna('').str.upper().str.contains('US')].copy()\n",
    "df_ariport_Code.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ariport_Code.groupby('type')['type'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph to show the percentage of the type of the Aripor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "port_type = df_ariport_Code.groupby('type')['type'].count()\n",
    "ax = (port_type[port_type>0]/df_ariport_Code.shape[0]*100).plot(kind='bar', title=f\"Percent of type of Airport %\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "excludedValues = ['closed', 'heliport', 'seaplane_base', 'balloonport']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, the Project target on the Airport so that some type of port is not correctly such as balloonport,helipori. Therefore, I will make filter out the some type of port that is unnecessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_airports = df_ariport_Code[~df_ariport_Code['type'].str.strip().isin(excludedValues)].copy()\n",
    "df_airports.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also verify that the municipality field is available for all airports\n",
    "df_airports[df_airports.municipality.isna()].head(5)\n",
    "df_airports = df_airports[~df_airports['municipality'].isna()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the municipality column to upper case in order to be able to join it with our other datasets.\n",
    "df_airports.municipality = df_airports.municipality.str.upper()\n",
    "df_airports.groupby('iso_region')['iso_region'].count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply len to the iso_region field to see which ones are longer than 5 characters since the field is a combination of US and state code\n",
    "df_airports['len'] = df_airports[\"iso_region\"].apply(len)\n",
    "# let's remove the codes that are incorrect.\n",
    "df_airports = df_airports[df_airports['len']==5].copy()\n",
    "# finally, let's extract the state code\n",
    "df_airports['state'] = df_airports['iso_region'].str.strip().str.split(\"-\", n = 1, expand = True)[1]\n",
    "df_airports.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gdf = geopandas.GeoDataFrame(\n",
    "    df_airports[['longitude','latitude']], \n",
    "    geometry=geopandas.points_from_xy(df_airports['longitude'], \n",
    "                                      df_airports['latitude']))\n",
    "world = geopandas.read_file(geopandas.datasets.get_path('naturalearth_lowres'))\n",
    "ax = world.plot(color='white', edgecolor='black', figsize=(16,12))\n",
    "gdf.plot(ax=ax, color='blue', markersize=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Demographic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demographics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demographics.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the city to upper case and remove any leading and trailing spaces\n",
    "df_demographics.City = df_demographics.City.str.upper().str.strip()\n",
    "df_demographics.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the missing values\n",
    "df_demographics.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_demographics.groupby('City')['City'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any leading or trailing spaces and convert to upper case\n",
    "df_demographics.City = df_demographics.City.str.strip().str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#primary key will be the combination of city name and race\n",
    "df_demographics[df_demographics[['City','Race']].duplicated()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_demographics[(df_demographics.City == 'WILMINGTON') & (df_demographics.Race == 'Asian')]\n",
    "df_demographics[df_demographics[['City', 'State','Race']].duplicated()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_demographics_null =  df_demographics.isnull().sum()\n",
    "ax = (_demographics_null[_demographics_null>0]/df_demographics.shape[0]*100).plot(kind='bar', title=f\"Percent Null of demographics %\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demographics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define the Data Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Conceptual Data Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I create a Star Schema for OLAP queries. The Schema for each table will following the bellow figure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Star Schema](./documents/i94_immagration.png \"Star Schema\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Mapping Out Data Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the I94_SAS_Labels_Descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the the I94_SAS_Labels_Description.SAS file to CSV files.\n",
    "The output files:\n",
    "- i94_country.csv\n",
    "- i94_model.csv\n",
    "- i94_port.csv\n",
    "- i94_state_addrl.csv\n",
    "- i94_visa.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loading,transformation,cleanup and create spark data table frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### fact_immagration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load the immigration data from the base dataset.\n",
    "- Convert for arrival and departure data column.\n",
    "- Convert for i94visa and i94mode data column.\n",
    "- Remove all entries into the united states that weren't via air travel.\n",
    "- Drop rows where the gender values entered is undefined.\n",
    "- Convert the arrival dates and departure dates into new columns.\n",
    "- Filter up the arrival modes.\n",
    "- Use an inner join to drop invalid codes country of citizenship and for countr of residence.\n",
    "- Add entry_port names and entry port states.\n",
    "- Compute the age of each individual.\n",
    "- Insert the immigration fact data into a spark dataframes.\n",
    "- Saving the data in parquet format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dim_airports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load the csv directly into a spark dataframes.\n",
    "- Equivalent to the following pandas code.\n",
    "- Verify that the municipality field is available for all airports.\n",
    "- Convert the municipality column to upper case in order to be able to join it with our other datasets.\n",
    "- Extract the state codes.\n",
    "- Convert the dataframes from pandas to spark.\n",
    "- Saving the data in parquet format in a spark dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dim_demographics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load the various csv files into pandas dataframes.\n",
    "- Convert the city to upper case and remove any leading and trailing spaces.\n",
    "- Remove any leading or trailing spaces and convert to upper case.\n",
    "- Primary key will be the combination of city name and race.\n",
    "- Convert the dataframes from pandas to spark.\n",
    "- Insert data into the demographics dim table.\n",
    "- Saving the data in parquet format.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dim_temperture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load data base abd use pandas to load DataFame.\n",
    "- Keep only data for the United States.\n",
    "- Convert the date to datetime objects.\n",
    "- Remove all dates prior to 1950.\n",
    "- Convert the city names to upper case.\n",
    "- Convert the dataframes from pandas to spark\n",
    "- Insert the temperature dim data into a spark dataframe.\n",
    "- Saving the data in parquet format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run Pipelines to Model the Data \n",
    "### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model. Refer and run etl.py script to create the data pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from etl import *\n",
    "print(\"Start the ETL process ......\")\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"Create the Spark Session...\")\n",
    "spark = create_Spark()\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"Extraxting SAS dataset...\")\n",
    "get_SAS_dataset()\n",
    "print(\"----------------------------------------------------------------\")\n",
    "main_etl(spark)\n",
    "print(\"Finish the ETL process\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quality_check import *\n",
    "print(\"Start the data quality_check process ......\")\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"Create the Spark Session...\")\n",
    "spark = create_Spark()\n",
    "print(\"----------------------------------------------------------------\")\n",
    "main_check(spark)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Data dictionary \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### fact_immagration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"tg\" align=\"left\">\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky\">Feature</th>\n",
    "    <th class=\"tg-0pky\">Description</th>\n",
    "  </tr>  \n",
    " <tr><td class=\"tg-0pky\">cicid</td><td class=\"tg-0pky\">Record ID</td></tr>\n",
    " <tr><td class=\"tg-0pky\">citizenship_country</td><td class=\"tg-0pky\"> The code for immigrant city of residence</td></tr>\n",
    " <tr><td class=\"tg-0pky\">residence_country</td><td class=\"tg-0pky\">The code for immigrant country of residence</td></tr>\n",
    " <tr><td class=\"tg-0pky\">city</td><td class=\"tg-0pky\">The city name of arrival</td></tr>\n",
    " <tr><td class=\"tg-0pky\">state</td><td class=\"tg-0pky\">US state of arrival</td></tr>\n",
    " <tr><td class=\"tg-0pky\">arrival_date</td><td class=\"tg-0pky\">Arrival Date in the USA</td></tr>\n",
    " <tr><td class=\"tg-0pky\">departure_date</td><td class=\"tg-0pky\">Departure Date from the USA </td></tr>\n",
    " <tr><td class=\"tg-0pky\">age</td><td class=\"tg-0pky\"> Age of Respondent in Years </td></tr>\n",
    " <tr><td class=\"tg-0pky\">visa_type</td><td class=\"tg-0pky\">\tThe codes of VISA</td> </tr>\n",
    " <tr><td class=\"tg-0pky\">detailed_visa_type</td><td class=\"tg-0pky\">The detailed about type of VISA</td></tr>\n",
    " <tr><td class=\"tg-0pky\">gender</td><td class=\"tg-0pky\">The immigrant sex</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### dim_demographics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"tg\" align=\"left\">\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky\">Feature</th>\n",
    "    <th class=\"tg-0pky\">Description</th>\n",
    "  </tr>\n",
    " <tr><td class=\"tg-0pky\">city</td><td class=\"tg-0pky\">City Name</td>\n",
    " <tr><td class=\"tg-0pky\">state</td><td class=\"tg-0pky\">US State where city is located</td>\n",
    " <tr><td class=\"tg-0pky\">median_age</td><td class=\"tg-0pky\">Median age of the population</td>\n",
    " <tr><td class=\"tg-0pky\">male_population</td><td class=\"tg-0pky\">Count of male population</td>\n",
    " <tr><td class=\"tg-0pky\">female_population</td><td class=\"tg-0pky\">Count of female population</td>\n",
    " <tr><td class=\"tg-0pky\">total_population</td><td class=\"tg-0pky\">Count of total population</td>\n",
    " <tr><td class=\"tg-0pky\">number_of_veterans</td><td class=\"tg-0pky\">Count of total Veterans</td>\n",
    " <tr><td class=\"tg-0pky\">toreign born</td><td class=\"tg-0pky\">Count of residents of the city that were not born in the city</td>\n",
    " <tr><td class=\"tg-0pky\">average_household_size</td><td class=\"tg-0pky\">Average city household size</td>\n",
    " <tr><td class=\"tg-0pky\">state_code</td><td class=\"tg-0pky\">US state code </td>\n",
    " <tr><td class=\"tg-0pky\">race</td><td class=\"tg-0pky\">Respondent race</td>\n",
    " <tr><td class=\"tg-0pky\">count</td><td class=\"tg-0pky\">Count of city's individual per race</td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### dim_airports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"tg\" align=\"left\">\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky\">Feature</th>\n",
    "    <th class=\"tg-0pky\">Description</th>\n",
    "  </tr>\n",
    "<tr><td class=\"tg-0pky\">ident</td><td class=\"tg-0pky\">Airport identity number </td></tr>\n",
    "<tr><td class=\"tg-0pky\">type</td><td class=\"tg-0pky\">Type of airport by size</td></tr>\n",
    "<tr><td class=\"tg-0pky\">name</td><td class=\"tg-0pky\"> Airport name</td></tr>    \n",
    "<tr><td class=\"tg-0pky\">elevation_ft</td><td class=\"tg-0pky\"> Elevation of airport in feet</td></tr>\n",
    "<tr><td class=\"tg-0pky\">iso_country</td><td class=\"tg-0pky\">Country of airport </td></tr>\n",
    "<tr><td class=\"tg-0pky\">iso_region</td><td class=\"tg-0pky\">Region of airport within country </td></tr>\n",
    "<tr><td class=\"tg-0pky\">municipality</td><td class=\"tg-0pky\"> Municipality of airport </td></tr>\n",
    "<tr><td class=\"tg-0pky\">gps_code</td><td class=\"tg-0pky\">GPS code </td></tr>\n",
    "<tr><td class=\"tg-0pky\">iata_code</td><td class=\"tg-0pky\">IATA code  </td></tr>\n",
    "<tr><td class=\"tg-0pky\">local_code</td><td class=\"tg-0pky\">Local identity code  </td></tr>\n",
    "<tr><td class=\"tg-0pky\">coordinates</td><td class=\"tg-0pky\">Longitude and Latitude of airport </td></tr>\n",
    "<tr><td class=\"tg-0pky\">len</td><td class=\"tg-0pky\">Length of the ident </td></tr>\n",
    "<tr><td class=\"tg-0pky\">state</td><td class=\"tg-0pky\">State code </td></tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### dim_temperture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"tg\" align=\"left\">\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky\">Feature</th>\n",
    "    <th class=\"tg-0pky\">Description</th>\n",
    "  </tr>\n",
    "<tr><td class=\"tg-0pky\">date</td><td class=\"tg-0pky\">Data time </td></tr>\n",
    " <tr><td class=\"tg-0pky\">city</td><td class=\"tg-0pky\">City name</td></tr>\n",
    " <tr><td class=\"tg-0pky\">average_temperature</td><td class=\"tg-0pky\">Average temperature of city</td></tr>    \n",
    " <tr><td class=\"tg-0pky\">average_termperature_uncertainty</td><td class=\"tg-0pky\">Uncertainty of Avg Temp of city</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The rationale for the choice of tools and technologies for the project:\n",
    "    - Pandas for loading CSV data files then process,clean and analyse data\n",
    "    - Spark for large datasets such as sas7bdat data. It's helpful for processing to extract,transform,load and store tables.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Propose how often the data should be updated:\n",
    "    - Because the raw data of immigration and temperature are built based on the month so that dataset should be update every month.\n",
    "    - For all tables during the update, it should proceed with \"append\" mode.\n",
    "* How often ETL script should be run:\n",
    "    - The ETL script should be run monthly basis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Suggestions for following scenarios:\n",
    "    * Data is 100x:\n",
    "        - The data would be stored in AWS S3 bucket or orther cloud storage.\n",
    "        - Spark platform can still use to process the data but it should enable parallel processing of the data.\n",
    "        - The AWS Redshift is good choose to store the data during processing of the ETL script running.\n",
    "    * Data is used in dashboard and updated every day 07:00AM:\n",
    "        - Use the Apache Airflow to perform the ETL and data qualtiy validation.\n",
    "        - The output data should be store and updated in cloud storage susch as AWS RDS to allow the dashboard to display the data all the time.\n",
    "    * The database needed to be accessed by 100+ people:\n",
    "        - In this case, the data should be migrated to the AWS Redshift to allow the auto-scaling capabilities.\n",
    "        - Immigration the Elastic Load Balancing of AWS to improve the performance of the dashboard application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
